{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3aad10",
   "metadata": {},
   "source": [
    "## CMSC 35300 Final Project: Lasso Models\n",
    "Shweta Kamath <br>\n",
    "Nivedita Vatsa <br>\n",
    "Carolyn Vilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae08929",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af30fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e77566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(\"data/all_data_standardized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ee0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out Xs\n",
    "X = df.loc[:, ~df.columns.isin([\"child_id\", \"mother_id\", \"treat_alike_scale\", \"treat_alike_binary\"])]\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Separate out two prospective ys\n",
    "y_scale = df.loc[:, df.columns == \"treat_alike_scale\"]\n",
    "y_scale = y_scale.to_numpy()\n",
    "\n",
    "y_binary = df.loc[:, df.columns == \"treat_alike_binary\"]\n",
    "y_binary = y_binary.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21568e4",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Predict repeatedly using cross validation; plot test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d191f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(self, learning_rate, iteration, regularization):\n",
    "        \"\"\"\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        \"\"\"\n",
    "        self.m = None\n",
    "        self.n = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.regularization = regularization # will be the l1/l2 regularization class according to the regression model.\n",
    "        self.lr = learning_rate\n",
    "        self.it = iteration\n",
    "\n",
    "    def cost_function(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        :param y: Original target value.\n",
    "        :param y_pred: predicted target value.\n",
    "        \"\"\"\n",
    "        return (1 / (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
    "    \n",
    "    def hypothesis(self, weights, bias, X):\n",
    "        \"\"\"\n",
    "        :param weights: parameter value weight.\n",
    "        :param X: Training samples.\n",
    "        \"\"\"\n",
    "        return np.dot(X, weights) #+ bias\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Target value should be in the shape of (n, 1) not (n, ).\n",
    "        # So, this will check that and change the shape to (n, 1), if not.\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError as e:\n",
    "            # we need to change it to the 1 D array, not a list.\n",
    "            print(\"ERROR: Target array should be a one dimentional array not a list\"\n",
    "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
    "                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n",
    "            return \n",
    "        \n",
    "        # m is the number of training samples.\n",
    "        self.m = X.shape[0]\n",
    "        # n is the number of features.\n",
    "        self.n = X.shape[1]\n",
    "\n",
    "        # Set the initial weight.\n",
    "        self.w = np.zeros((self.n , 1))\n",
    "\n",
    "        # bias.\n",
    "        self.b = 0\n",
    "\n",
    "        for it in range(1, self.it+1):\n",
    "            # 1. Find the predicted value through the hypothesis.\n",
    "            # 2. Find the Cost function value.\n",
    "            # 3. Find the derivation of weights.\n",
    "            # 4. Apply Gradient Decent.\n",
    "            y_pred = self.hypothesis(self.w, self.b, X)\n",
    "            #print(\"iteration\",it)\n",
    "            #print(\"y predict value\",y_pred)\n",
    "            cost = self.cost_function(y, y_pred)\n",
    "            #print(\"Cost function\",cost)\n",
    "            # fin the derivative.\n",
    "            dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
    "            #print(\"weights derivation\",dw)\n",
    "            #db = -(2 / self.m) * np.sum((y_pred - y))\n",
    "\n",
    "            # change the weight parameter.\n",
    "            self.w = self.w - self.lr * dw\n",
    "            #print(\"updated weights\",self.w)\n",
    "            #self.b = self.b - self.lr * db\n",
    "\n",
    "            if it % 10000 == 0:\n",
    "                print(\"The Cost function for the iteration {}----->{}\".format(it, cost))\n",
    "                \n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        :param test_X: feature values to predict.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
    "\n",
    "        y_pred = self.hypothesis(self.w, self.b, test_X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0660e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class l1_regularization:\n",
    "    '''Regularization used for Lasso Regression'''\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        '''This will be retuned when we call this class.'''\n",
    "        return self.lamda * np.sum(np.abs(weights))\n",
    "    \n",
    "    def derivation(self, weights):\n",
    "        \"Derivation of the regulariozation function.\"\n",
    "        return self.lamda * np.sign(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4f2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class l2_regularization:\n",
    "    '''Regularization used for Ridge Regression'''\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        \"This will be retuned when we call this class.\"\n",
    "        return self.lamda * np.sum(np.square(weights))\n",
    "    \n",
    "    def derivation(self, weights):\n",
    "        \"Derivation of the regulariozation function.\"\n",
    "        return self.lamda * 2 * (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c681a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(Regression):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, lamda, learning_rate, iteration):\n",
    "        '''\n",
    "        Define the hyperparameters we are going to use in this model.\n",
    "        :param lamda: Regularization factor.\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        '''\n",
    "        self.regularization = l1_regularization(lamda)\n",
    "        super(LassoRegression, self).__init__(learning_rate, iteration, self.regularization)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        '''\n",
    "        return super(LassoRegression, self).train(X, y)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        '''\n",
    "        parma test_X: Value need to be predicted.\n",
    "        '''\n",
    "        return super(LassoRegression, self).predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ccd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(Regression):\n",
    "    '''\n",
    "    Ridge Regression is one of the variance of the Linear Regression. This model doing the parameter learning \n",
    "    and regularization at the same time. This model uses the l2-regularization. \n",
    "    This is very similar to the Lasso regression.\n",
    "    * Regularization will be one of the soluions to the Overfitting.\n",
    "    * Overfitting happens when the model has \"High Variance and low bias\". So, regularization adds a little bias to the model.\n",
    "    * This model will try to keep the balance between learning the parameters and the complexity of the model( tries to keep the parameter having small value and small degree of palinamial).\n",
    "    * The Regularization parameter(lamda) controls how severe  the regularization is. \n",
    "    * large lamda adds more bias , hence the Variance will go very small --> this may cause underfitting(Low bias and High Varinace).\n",
    "    * Lamda can be found by tial and error methos. \n",
    "    '''\n",
    "    def __init__(self, lamda, learning_rate, iteration):\n",
    "        \"\"\"\n",
    "        Define the hyperparameters we are going to use in this model.\n",
    "        :param lamda: Regularization factor.\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        \"\"\"\n",
    "        self.regularization = l2_regularization(lamda)\n",
    "        super(RidgeRegression, self).__init__(learning_rate, iteration, self.regularization)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        return super(RidgeRegression, self).train(X, y)\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        parma test_X: Value need to be predicted.\n",
    "        \"\"\"\n",
    "        return super(RidgeRegression, self).predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a78aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_scale_labels(y_scale_pred):\n",
    "    '''\n",
    "    Assign label 1 through 4\n",
    "    \n",
    "    arg:\n",
    "    - y_scale_pred: vector of predicted y values\n",
    "    \n",
    "    Returns: vector of predicted y labels (1 through 4)\n",
    "    '''\n",
    "    # generate nx4 matrix where each column is 1,2,3,4\n",
    "    m, _ = y_scale_pred.shape\n",
    "    mat = np.tile(np.arange(4)+1, (m,1))\n",
    "    \n",
    "    # find lowest absolute distance\n",
    "    return np.argmin(abs(mat - y_scale_pred), axis=1).reshape(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492cbf09",
   "metadata": {},
   "source": [
    "### Estimate Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44adff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cost function for the iteration 1000----->0.41679908332692683\n",
      "The Cost function for the iteration 2000----->0.28518990027337116\n",
      "The Cost function for the iteration 3000----->0.24278528202110722\n",
      "The Cost function for the iteration 4000----->0.22808387943344677\n",
      "The Cost function for the iteration 5000----->0.22214167586702094\n",
      "The Cost function for the iteration 6000----->0.21910046089054594\n",
      "The Cost function for the iteration 7000----->0.2171291057448461\n",
      "The Cost function for the iteration 8000----->0.21563866925776792\n",
      "The Cost function for the iteration 9000----->0.21442582462138762\n",
      "The Cost function for the iteration 10000----->0.21340903572559092\n",
      "The Cost function for the iteration 11000----->0.21254699258617635\n",
      "The Cost function for the iteration 12000----->0.21181313305741853\n",
      "The Cost function for the iteration 13000----->0.21118746236086477\n",
      "The Cost function for the iteration 14000----->0.21065374029978462\n",
      "The Cost function for the iteration 15000----->0.21019836200157604\n",
      "The Cost function for the iteration 16000----->0.20980979653782844\n",
      "The Cost function for the iteration 17000----->0.20947822890350096\n",
      "The Cost function for the iteration 18000----->0.2091952917158196\n",
      "The Cost function for the iteration 19000----->0.20895384771255265\n",
      "The Cost function for the iteration 20000----->0.20874780767971488\n",
      "The Cost function for the iteration 21000----->0.2085719761768841\n",
      "The Cost function for the iteration 22000----->0.20842192023329303\n",
      "The Cost function for the iteration 23000----->0.20829385741406123\n",
      "The Cost function for the iteration 24000----->0.20818456034347205\n",
      "The Cost function for the iteration 25000----->0.20809127524860266\n",
      "The Cost function for the iteration 26000----->0.2080116524592203\n",
      "The Cost function for the iteration 27000----->0.20794368710737296\n",
      "The Cost function for the iteration 28000----->0.20788566852929224\n",
      "The Cost function for the iteration 29000----->0.20783613709239426\n",
      "The Cost function for the iteration 30000----->0.207793847357724\n",
      "The Cost function for the iteration 31000----->0.2077577366481311\n",
      "The Cost function for the iteration 32000----->0.20772689822890722\n",
      "The Cost function for the iteration 33000----->0.20770055842402935\n",
      "The Cost function for the iteration 34000----->0.2076780570904772\n",
      "The Cost function for the iteration 35000----->0.20765883095784507\n",
      "The Cost function for the iteration 36000----->0.2076423994127804\n",
      "The Cost function for the iteration 37000----->0.20762835236948377\n",
      "The Cost function for the iteration 38000----->0.20761633992015058\n",
      "The Cost function for the iteration 39000----->0.20760606350415778\n",
      "The Cost function for the iteration 40000----->0.20759726837312728\n",
      "The Cost function for the iteration 41000----->0.2075897371617027\n",
      "The Cost function for the iteration 42000----->0.2075832844017824\n",
      "The Cost function for the iteration 43000----->0.20757775184175958\n",
      "The Cost function for the iteration 44000----->0.20757300445264126\n",
      "The Cost function for the iteration 45000----->0.20756892702024807\n",
      "The Cost function for the iteration 46000----->0.20756542123749178\n",
      "The Cost function for the iteration 47000----->0.20756240322334576\n",
      "The Cost function for the iteration 48000----->0.20755980140589436\n",
      "The Cost function for the iteration 49000----->0.2075575547160338\n",
      "The Cost function for the iteration 50000----->0.207555611046238\n",
      "The Cost function for the iteration 51000----->0.20755392593549285\n",
      "The Cost function for the iteration 52000----->0.2075524614472099\n",
      "The Cost function for the iteration 53000----->0.2075511852117998\n",
      "The Cost function for the iteration 54000----->0.2075500696097441\n",
      "The Cost function for the iteration 55000----->0.20754909107454625\n",
      "The Cost function for the iteration 56000----->0.20754822949797144\n",
      "The Cost function for the iteration 57000----->0.20754746772256435\n",
      "The Cost function for the iteration 58000----->0.20754679110863755\n",
      "The Cost function for the iteration 59000----->0.20754618716480158\n",
      "The Cost function for the iteration 60000----->0.20754564523271332\n",
      "The Cost function for the iteration 61000----->0.20754515621808606\n",
      "The Cost function for the iteration 62000----->0.20754471236117175\n",
      "The Cost function for the iteration 63000----->0.2075443070409246\n",
      "The Cost function for the iteration 64000----->0.20754393460790235\n",
      "The Cost function for the iteration 65000----->0.20754359024168828\n",
      "The Cost function for the iteration 66000----->0.20754326982923582\n",
      "The Cost function for the iteration 67000----->0.20754296986106602\n",
      "The Cost function for the iteration 68000----->0.20754268734269662\n",
      "The Cost function for the iteration 69000----->0.20754241971906925\n",
      "The Cost function for the iteration 70000----->0.2075421648100661\n",
      "The Cost function for the iteration 71000----->0.20754192075548916\n",
      "The Cost function for the iteration 72000----->0.20754168596811395\n",
      "The Cost function for the iteration 73000----->0.20754145909363172\n",
      "The Cost function for the iteration 74000----->0.2075412389764705\n",
      "The Cost function for the iteration 75000----->0.20754102463063143\n",
      "The Cost function for the iteration 76000----->0.20754081521480458\n",
      "The Cost function for the iteration 77000----->0.20754061001113686\n",
      "The Cost function for the iteration 78000----->0.2075404084071152\n",
      "The Cost function for the iteration 79000----->0.20754020988010863\n",
      "The Cost function for the iteration 80000----->0.20754001398417812\n"
     ]
    }
   ],
   "source": [
    "param = {\"lamda\" : 0,\n",
    "         \"learning_rate\" : 10**(-10),\n",
    "         \"iteration\" : 800000}\n",
    "\n",
    "lasso = LassoRegression(**param)\n",
    "lasso.train(X, y_scale)\n",
    "\n",
    "# predict\n",
    "y_scale_pred = lasso.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23ed251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---most important features---\n",
      "year_at_y : 0.00036801\n",
      "yob_child : 0.00036552\n"
     ]
    }
   ],
   "source": [
    "X_names = df.columns[(~df.columns.isin([\"child_id\", \"mother_id\", \"treat_alike_scale\", \"treat_alike_binary\"]))]\n",
    "\n",
    "filt = (np.round(lasso.w[1:], 4)>0.0).flatten()\n",
    "b_names = X_names[filt]\n",
    "b_vals  = lasso.w[1:][filt]\n",
    "\n",
    "print(\"---most important features---\")\n",
    "for n, v in zip(b_names, b_vals):\n",
    "    print(n, \":\", '%.08f' % v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7bc381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---error rate---\n",
      "---mean square error ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4151945204879751"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---error rate---\")\n",
    "y_scale_pred_label = predict_scale_labels(y_scale_pred)\n",
    "100*np.sum(y_scale != y_scale_pred_label)/len(y_scale)\n",
    "\n",
    "print(\"---mean square error ---\")\n",
    "mse = (np.square(y_scale - y_scale_pred)).mean(axis=None)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d97e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'Prediction by Label')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhEklEQVR4nO3de5RcZZnv8e+vOw0BggaoCIQGMksQ4XjC5UQuKkiQDjQD6uB4YY1aOjAkByeMC4eZMy5GRkRHz/LM8hAvhEGgRAdH8RYdWtI6QGDJxUBC5OYkZyaYTgjpCkQu4dKdfs4fVd12Nd2V6lTt7NrVv89atein9+6qp1/S9dR72e9WRGBmZlNbW9oJmJlZ+lwMzMzMxcDMzFwMzMwMFwMzM8PFwMzMcDGwFiLpJklXl78+VdJvd/F5rpX0943NDiT9g6RvN/p5J5nDnZIu2t0/a83PxcB2K0nrJb0k6QVJT0u6UdKMRr9ORNwdEUfVkM/HJN0z5mcXRcTnGp1TI5Xb8cy087DW4WJgaTgvImYAJwBvBa4Ye4Kkabs9K7MpzMXAUhMRG4Ee4C0AkkLSJyStBdaWv3eupNWStkn6laS5wz8v6XhJD0l6XtK/AtNHHTtdUt+o+FBJP5TUL2mrpK9KOhq4Fjil3FPZVj53ZLipHP+FpHWSnpG0TNLsUcdC0iJJayU9K+lrklTl154u6V/LOT8k6djy81wu6QejT5S0RNJXJtOmkvaT9LPy7/ls+evOMae9UdIDkn4v6SeS9h/18yeX23mbpIclnT6Z17fscjGw1Eg6FDgHWDXq2+8FTgKOkXQCcAOwEDgAWAosk7SnpD2AHwM3A/sD3wfeN8HrtAM/A54E5gCHAN+NiMeBRcC9ETEjImaO87NnAP8IfAA4uPwc3x1z2rmUejjHls87q8qv/Z5yrvsD/wL8WFIH8G3gbEkzy687Dfhg+febjDbgRuBw4DDgJeCrY875KPDnwGxgELim/JqHAP8GXF3O76+BH0iaNckcLINcDCwNPy5/Cr8HuAv4wqhj/xgRz0TES8BfAEsj4v6I2BERBeAV4OTyowP4SkQMRMStwK8neL0TKb3xXR4RL0bEyxFxzwTnjvVnwA0R8VBEvAL8HaWexJxR53wxIrZFxO+AO4DjqjzfgxFxa0QMAP9EqTdzckQ8BawA3l8+72ygGBEP1pgnABGxNSJ+EBHbI+J54PPAO8ecdnNEPBIRLwJ/D3ygXDA/DNwWEbdFxFBE9AIrKRVsa3EuBpaG90bEzIg4PCIuKb/xD9sw6uvDgU+Vhyy2lQvIoZTe2GcDG6Nyp8UnJ3i9Q4EnI2JwF3KdPfp5I+IFYCul3sWwzaO+3g5UmxAf+f0iYgjoK78GQIHSGzLl/062V4CkvSUtlfSkpOcoFZiZ5Tf71+RA6XfrAHKU2vv9Y9r7HZR6RNbiXAys2Yx+c98AfL5cOIYfe0fELcBTwCFjxucPm+A5NwCHTTApvbNtezdRepMEQNI+lIasNu7sF5nAoaOeqw3oLL8GlIa95kp6C6Whp+/swvN/CjgKOCkiXgecNvxy4+VAqc0GgCKldrp5THvvExFf3IU8LGNcDKyZ/TOwSNJJKtlH0h9L2he4l9J496WSpkk6n9Jw0HgeoFQ8vlh+jumS3l4+9jTQWZ6DGM+/AB+XdJykPSkNad0fEet38Xf6H5LOLxemT1Ia9roPICJeBm4tv+YD5WGnajrKv8vwYxqwL6V5gm3lieErx/m5D0s6RtLewFXArRGxg9K8xXmSzpLUXn7O08eZgLYW5GJgTSsiVlKaN/gq8CywDvhY+dirwPnl+FlKk60/nOB5dgDnAUcAv6M0NPPB8uF/Bx4FNksqjvOzv6Q0rv4DSgXljcCH6vi1flJ+7WeBjwDnl+cPhhWA/05tQ0S3UXrjH378A/AVYC9Kn/TvA34+zs/dDNxEaXhrOnApQERsoDTB/Wmgn1JP4XL8PjElyDe3MWsekg4DngAOiojn0s7Hpg5XfLMmUZ5DuIzSslcXAtutfJWnWRMoT0w/TWl1z9kpp2NTkIeJzMzMw0RmZpbRYaJcLhdz5sxJOw0zs0x58MEHixEx7vYimSwGc+bMYeXKlWmnYWaWKZImukrfw0RmZpZwMZB0g6Qtkh6Z4Pjp5W10V5cfn0kyHzMzG1/Sw0Q3Ubp69FtVzrk7Is5NOA8zM6si0Z5BRKwAnknyNczMrH7NMGdwSvmOSj2S/lvayZiZTUVpryZ6CDg8Il6QdA6lLXyPHO9ESRcDFwMcdthEOxWbmdmuSLVnEBHPlW8WQkTcRmlL3twE514XEfMiYt6sWb4Ln5lZI6XaM5B0EPB0RISkEykVp61p5gRwzTXXsG7durqeo6+vdC/2zs76toI/4ogjuPTSS+t6DjOznUm0GEi6BTgdyEnqo3SjjQ6AiLgW+FPgf0oapLQf+4eiRTZLeumll3Z+kplZk8jkRnXz5s2LZr8CefjT/DXXXJNyJmZmJZIejIh54x1LewLZpoBmGXbzkJvZxFwMLBM87GZJaJYPKpD+hxUXA0tcI/6Be9jtD+p9A2uVN69m0SofVFwMzKaYVnnzagR/UPkDFwOzjKn3DaxV3ryssZphOwozM0uZewZmllmNmACu19q1a4HGDDnVq555HBcDq6oZ/tigef7g6p00bYb2bJa2hPrbc926dTyxejUHNTCnyRoeXtm2enWKWcDmOn++5YpBM/yxQfP8wTXij+0/HnmIw2bsaGBWk7fHQOlP7uX1v04th9+90F73c6xbt45Vj66CmfXns8uGSv9ZtXFVikkA2xrzNAcBF6LGPFmGfZP6LiBuuWKwbt06Vv3mMYb23j/VPPRq6X/Mg/+v3nq969q2N+ZWEofN2MEV815oyHNl2dUrZ9T9HMPLOlNV/6/RMPW2R19fH89T/xthK3gKeKGO9my5YgAwtPf+vHyMb542/bGfpZ2CmWVESxYDa5y+vj5efL69IZ+Ks+7J59vZp85Psp2dnfSrn6HThxqUVXa13dlG5yH1XfjW2dnJtmLRw0SUekcz67iQsOWKQV9fH23bf+9PxUDb9q309Q2mnYaZZUDLFQNrrM7OTl4efMpzBpTmDKbXuYUDANtKn4pTM/y/Mu3O3jbgkJRzsBEtVww6Ozt5+pVpnjOgNGfQ2Znmojsb64gjjkg7hZGVbkceMu4dZnefQxrTHptJdwJ5+G5cB6SWQclm6luk1nLFwKyZpb3UeHQOrbAdRTMU1/5ycZ15ZLrFdSb1tYeLgZlllotr47gY2E797oX0VxM9vb00xn7g3umtwvndC+28KbVXN0uWi4FV1QzdcIBXy13x6XPS64q/ieZpD7NGa8li0Lb9mdSXlurl5wCI6a9LLYfSFcj1TSA3QzccWqcrbtasWq4YNMsnt7VrnwfgyDemuZrnoKZpDzNrbi1XDPxJ1qy6gYEB1q9fz9atWznggLQXRFqz8M1tzKaYTZs28eKLL7J06dK0U7EmkmjPQNINwLnAloh4S5Xz3grcB3wwIm5NMiezrKtnm/aBgQGeffZZAH7+85+zYcMGOjo6dum56t0evRk0Ysv7Rm1Xn3Z7Jt0zuAk4u9oJktqBLwG3J5yL2ZS3adOmqrFN3l577cVee+2Vdhp1S7RnEBErJM3ZyWmLgR8Ab00yF7NWUc+nxzPOOKMifv7556f0vFbWezaNlOqcgaRDgD8Brq3h3IslrZS0sr+/P/nkzFpQRFSNbepKewL5K8DfRsRO76kYEddFxLyImDdr1qzkMzNrQaeeempFfNppp6WUiTWbtJeWzgO+KwkgB5wjaTAifpxqVmYtas8996wa29SVas8gIv4oIuZExBzgVuASFwIbT39/P6tXr2bZsmVpp5Jpd999d0W8YsWKlDJpHcVikcWLF7N169adn9zEEi0Gkm4B7gWOktQn6UJJiyQtSvJ1rfVs3LgRgC9/+cspZ5JtXV1dTJtWGhCYNm0aCxYsSDmj7CsUCqxZs4ZCoZB2KnVJejXRBZM492MJpmIpqnct99gFAxdccAG7Mm+U9jruZpDP5+np6QGgvb2dfD6fckbZViwW6enpISLo6ekhn89n9qrutCeQzXZquFcwUWy1y+VyzJ8/H4D58+dn9o2rWRQKBXbsKK1/GRwczHTvIO0JZJsC6v00Pt6Kl6m8Nt6aR29v70gx2LFjB8uXL+eyyy5LOatd455BQjZv3szq1au55ZZb0k7FbESxWOSOO+4A4I477sj8pGfaTjzxxIr4pJNOSimT+rkYJGTz5s0AfOMb30g5E7M/KBQKIxeaDQ0NZXpYoxk89thjFfGjjz6aUib18zDROOqd8BwuBMM+8IEPcNBBu3ZfA096WiP19vYyMDAAlDaty/KwRjPYsmVL1ThL3DNIwNhiMDY2S4uXltpE3DMYhyc8rVXl83l++tOfAqVhIi8ttWHuGZhNMd6crnHa2tqqxlmS3czNbNI8gdxYXV1dFXGWh91cDMymkOXLl1fEt9/ue0rVY+HChVXjLHExsKY3Y8aMinjfffdNKZPsO/DAA6vGNnnlXZdH/ptVLgbW9F566aWKePv27Sllkn1PP/101dgmp1Ao0N7eDpT2esrysJuLgTW94cv9J4qtdgsWLKj4JHvWWWelnFG29fb2Mjg4CJT2Jho7DJclLgZmU0g+n6ejowOAjo4OLy2tU1dXV0V7egLZLEFjx2KzPjabptG7lp5xxhnetbRO+Xx+5N9jW1tbpouri4E1vTe/+c0V8dFHH51SJmaVWmlLcBcDa3qPP/54RTx2czCrnXcttYm4GJhNIYVCgaGhIaA0EZ/l1S/NoJWKq4uB2RTSSqtfmkErXdHtYmA2hZx66qkV8XibKlrtxtsSPKtcDKzpDV/UM1FslhYvLTXbjXzRWePcfffdFfGKFStSyqQ1eGmpmWWSb27TWLlcju7ubiTR3d3tpaUTkXSDpC2SHpng+HskrZG0WtJKSe9IMh+zqS6fz4/suZ/1T7LNIp/PM3fu3My3ZdI9g5uAs6sc/yVwbEQcB/w5cH3C+ZhNablcjtmzZwMwe/bsTH+SbRa5XI4lS5Zkvi0TLQYRsQJ4psrxF+IPt13aB/AtmOw13vCGN1TE3nZ51xWLRTZu3AjApk2bMr0u3hor9TkDSX8i6Qng3yj1DiY67+LyUNLK/v7+3ZegpW7RokUV8SWXXJJSJtk3eh18RGR6Xbw1VurFICJ+FBFvBt4LfK7KeddFxLyImDdr1qzdlp+l78Ybb6yIr7/eo4m7qpXWxVtjpV4MhpWHlN4oKZd2LtZcNmzYUDW22rXSunhrrFSLgaQjVF6kK+kEYA/Ag5hmCWmldfHWWEkvLb0FuBc4SlKfpAslLZI0PAj8PuARSauBrwEfHDWhbGYN1krr4q2xpiX55BFxwU6Ofwn4UpI5mFmlfD7P+vXr3SuwCk0zZ2A2kVNOOaUiftvb3pZSJq2hVdbFW2O5GFjTu/DCCyviiy66KKVMzFqXi4E1vW9/+9sV8c0335xSJmaty8XAmt6dd95ZEQ/fWcrMGsfFwMzMXAys+Q1vrDZRbGb1czGwpvemN72pIj7qqKNSysSsdbkYWNN74IEHKuL7778/pUzMWpeLgTW9rq6ukfset7e3ez8dswS4GFjTy+fzI8Vg2rRpvnLWLAEuBtb0crkc8+fPB2D+/Pm+ctYsAS4GlgmvvPJKxX/NrLFcDKzpFYtF7rrrLgDuuusu36rRLAEuBtb0li5dyvDO5kNDQyxdujTljMxaj4uBNb1f/OIXFXFvb29KmZi1LheDBOy5555VY5ucHTt2VI3NrH4uBgkYO8npSU+z1lUsFlm8eHHm57JcDKzp7bPPPlVjszQVCgXWrFlDoVBIO5W6uBhY0/vsZz9bEV999dUpZWJWqVgs0tPTQ0TQ09OT6d6Bi4E1vZkzZ1bEr3/969NJxGyMQqFQsdIty72DadUOSloCxETHI+LShmdkNsbYnsGVV17Jd77znZSyMfuD3t5eBgYGABgYGGD58uVcdtllKWe1a3bWM1gJPAhMB04A1pYfxwFe0jGBadOmVY1tcjZs2FA1NktLV1cXHR0dAHR0dGR6E8WqxSAiChFRAI4E5kfEkohYAryLUkGwcQwODlaNzdLUKqtfmkE+n0cSAG1tbZneRLHWOYPZwL6j4hnl71Ul6QZJWyQ9MsHxP5O0pvz4laRja8ynqU2fPr1qbJNz8MEHV8S+01l9WmX1SzPI5XJ0d3cjie7u7kxvolhrMfgisErSTZJuAh4CvlDDz90EnF3l+H8B74yIucDngOtqzKepvfzyy1Vjm5xnnnmmIvYn2l3XSqtfmkU+n2fu3LmZ7hVAjcUgIm4ETgJ+VH6cUh4+2tnPrQCeqXL8VxHxbDm8D+isJR+bWobvZTBRbLVrpdUvzSKXy7FkyZJM9wqgxmKg0qDYmcCxEfETYA9JJzY4lwuBnio5XCxppaSV/f39DX5pa2bbt2+vGlvtxlv9Yga1DxN9HTgFuKAcPw98rVFJSJpPqRj87UTnRMR1ETEvIubNmjWrUS9tNqW00uoXa6xai8FJEfEJ4GWA8tDOHo1IQNJc4HrgPRHhAUx7jbETxp5A3nWttPrFGqvWYjAgqZ3yBWiSZgFD9b64pMOAHwIfiYj/qPf5msUee+xRNbbJueqqqypib0ex61pp9Ys1Vq1XQ11DaeL4DZI+D/wpcMXOfkjSLcDpQE5SH3Al0AEQEdcCnwEOAL5e/rQyGBHzJvk7NJ1XX321amyTs//++1fE++23X0qZtIZ8Ps/69evdK7AKOy0GktooLQH9G0oXmwl4b0Q8vrOfjYgLdnL8IuCi2lK1qWrsnc2WLl3Kpz/96ZSyyb7h1S9mo+10mCgihoD/ExFPRMTXIuKrtRQCs0bxnc7MklfrnMFySe/T8MyT2W409p+d/xmaNV6txeAy4PvAq5KeLz+eSzCvTPObV2Mde2zlLiXHH398SpmYta6aJpAjYt+dn2XDJI1c5Tkc26574oknKuLHHnsspUzMWlfNeytLOh94B6XlpXdHxI+TSspstBdffLFqbGb1q3U7iq8Di4DfAI8AiyQ17ArkVjO6VzBebJMzY8aMqrGZ1a/WOYN3AmdFxI3lTevOoXT9gI3DxaCxxt456vLLL08pE7PWVWsx+C1w2Kj4UGBN49NpDZ5AbqyHH364Il61alVKmZi1rlqLwQHA45LulHQn8BgwS9IyScsSyy6jhjcCmyi2yRl7XYF32jRrvFonkD+TaBYtpru7m5/85Ccj8TnnnJNiNtnX1dXFbbfdxsDAgHfaNEtIrTe3uavaQ9K9SSeaJeedd15F/O53vzulTFqDd9o0S16tw0Q745v8jvL973+/Iv7e976XUiatwTttmiWvUcXAy2VG8Rh347XKfWbNmlWjioGN4qWlZpY1tV509peSqm0i77WTo7S1tVWNbfKWLl3Kww8//JrtrM2sMWp9lzoI+LWk70k6e5zdSz/S4Lwy7dRTT62ITzvttJQyaQ3FYnFk6G358uVs3eq7o5o1Wq2ria4AjgS+CXwMWCvpC5LeWD7+SGIZ2pS3dOlShoZKd1kdGhpy78AsATWPX0Rp4Htz+TEI7AfcKul/J5RbZt19990V8YoVK1LKpDX45jZmyavpojNJlwJ5oAhcD1weEQPlW2KupXRLTCsb/hQ7UWyT4+09zJJXa88gB5wfEWdFxPcjYgBGbol5bmLZmQHvete7KuIzzzwzpUzMWletcwafiYgnJzjm+yGPMX369KqxTc7ChQurxmZWP695TMBLL71UNbbJG16e62W6ZslI9C9L0g2Stkgad7WRpDdLulfSK5L+OslcLLsKhUJFMSgUCilnZNZ6kv6YdRNwdpXjzwCXAl9OOI/dqr29vWpsk9Pb28vg4CAAg4OD3t7DLAGJFoOIWEHpDX+i41si4tfAQJJ57G47duyoGtvkdHV1jawgkuQtrM0SkJkBWEkXS1opaWV/f3/a6VQ1Z86cqrFNznnnnTeyv1NEeEtwswRkphhExHURMS8i5s2aNSvtdKq64oorKuLPfMb3BqrHT3/604qewbJlvrmeWaNlphjY1NXb21vRM/CcgVnjuRgk4Oqrr66Ir7rqqpQyaQ3e+M8sebXeA3mXSLoFOB3ISeoDrgQ6ACLiWkkHASuB1wFDkj4JHBMRzyWZV9LWr19fNbbJeeWVV6rGZla/RItBRFywk+Obgc4kc7Ds88Z/ZsnzMJE1PW9UZ5Y8FwNret6ozix5LgYJOProoyviY445JqVMWsPChQsrtqPwRnVmjedikIADDzywamyTk8vl6OrqAmDBggUccMABKWdk1noSnUCequ65556KeOwEqE3ewoUL2bx5s3sFZglxMUjA8AVSE8U2eblcjiVLlqSdhlnL8jBRAjxM1HjFYpHFixezdevWtFMxa0kuBgnYvHlz1dgmr1AosGbNGt/LwCwhLgYJGBoaqhrb5BSLRXp6eogIenp63DswS4CLQQJ8c5vGKhQKIwV1x44d7h2YJcDFIAG5XK5qbJPjO52ZJc/FIAFPP/101dgmx7uWmiXPxcDMzFwMknDwwQdXxLNnz04pk9bgXUvNkudikICjjjqqamyT09XVxbRppesjp02bxoIFC1LOyKz1uBgk4IEHHqiI77///pQyaQ35fH5ko7r29nby+XzKGZm1HheDBHR1dY0sJ21vb/cn2Trlcjm6u7uRRHd3tzeqM0uAi0EC8vl8RTHwJ9n65fN55s6d67Y0S4iLQQJyudzIpPHs2bP9SbYBhjeqc1uaJcPFIAHFYpGNGzcCsHHjRm+fYGZNz8UgAYVCoeKKWW+fYGbNzsUgAcuXLx+5h0FEcPvtt6eckZlZdS4GCfD9DMwsaxItBpJukLRF0iMTHJekayStk7RG0glJ5rO7eG8iM8uapHsGNwFnVzneDRxZflwMfCPhfHaLsRupvfOd70wpEzOz2iRaDCJiBfBMlVPeA3wrSu4DZko6uMr5ZmaWgLTnDA4BNoyK+8rfew1JF0taKWllf3//bkluV43dSO2uu+5KKRMzs9qkXQw0zvdivBMj4rqImBcR82bNmpVwWvXxBLKZZU3axaAPOHRU3AlsSimXhvEEspllTdrFYBnw0fKqopOB30fEUynnVLcFCxYglTo9kjjrrLNSzsjMrLqkl5beAtwLHCWpT9KFkhZJWlQ+5TbgP4F1wD8DlySZz+6Sz+fp6OgAoKOjw5urmVnTm5bkk0fEBTs5HsAnkswhDcNbLi9btoxzzjnHm6uZWdNLtBhMZfl8nvXr17tXYGaZ4GKQkOEtl83MsiDtCWQzM2sCLgZmZuZiYGZmLgaWEcVikcWLF/uucWYJcTGwTCgUCqxZs8Z3jTNLiIuBNb1isUhPTw8RQU9Pj3sHZglwMbCmVygURm4jOjQ05N6BWQJcDKzp9fb2MjAwAMDAwADLly9POSOz1uNiYE2vq6urYq+nBQsWpJyRWetxMbCml8/nR3aBbWtr8xYfZglwMbCmN7zxnyS6u7u98Z9ZArw3kWWCN/4zS5aLgWWCN/4zS5aHiczMzMXAzMxcDMzMDBcDMzPDxcDMzHAxMDMzXAzMzAwXAzMzYzcUA0lnS/qtpHWS/tc4x/eT9CNJayQ9IOktSedkZmaVEi0GktqBrwHdwDHABZKOGXPap4HVETEX+Cjwf5PMyczMXivpnsGJwLqI+M+IeBX4LvCeMeccA/wSICKeAOZIOjDhvMzMbJSki8EhwIZRcV/5e6M9DJwPIOlE4HCgc+wTSbpY0kpJK/v7+xNK18xsakq6GGic78WY+IvAfpJWA4uBVcDga34o4rqImBcR82bNmtXwRM3MprKkdy3tAw4dFXcCm0afEBHPAR8HUOkOJv9VfpiZ2W6SdM/g18CRkv5I0h7Ah4Blo0+QNLN8DOAiYEW5QJiZ2W6SaM8gIgYl/SVwO9AO3BARj0paVD5+LXA08C1JO4DHgAuTzMnMzF4r8ZvbRMRtwG1jvnftqK/vBY5MOg8zM5uYr0A2MzMXg6QUi0UWL17M1q1b007FzGynXAwSUigUWLNmDYVCIe1UzMx2ysUgAcVikZ6eHiKCnp4e9w7MrOm5GCSgUCgQUbq2bmhoyL0DM2t6LgYJ6O3tZWBgAICBgQGWL1+eckZmZtW5GCSgq6uLjo4OADo6OliwYEHKGZmZVedikIB8Pk9pZw1oa2sjn8+nnJGZWXUuBgnI5XJ0d3cjie7ubg444IC0UzIzqyrxK5Cnqnw+z/r1690rMLNMcDFISC6XY8mSJWmnYWZWEw8TmZmZi4GZmbkYmJkZLgZmZgZoeNuELJHUDzyZdh41yAHFtJNoIW7PxnFbNlZW2vPwiBj3JvKZLAZZIWllRMxLO49W4fZsHLdlY7VCe3qYyMzMXAzMzMzFIGnXpZ1Ai3F7No7bsrEy356eMzAzM/cMzMzMxcDMzHAxqJukGyRtkfTIBMcl6RpJ6yStkXTC7s4xKyQdKukOSY9LelTSX41zjtuzRpKmS3pA0sPl9vzsOOe4PSdBUrukVZJ+Ns6xTLeli0H9bgLOrnK8Gziy/LgY+MZuyCmrBoFPRcTRwMnAJyQdM+Yct2ftXgHOiIhjgeOAsyWdPOYct+fk/BXw+ATHMt2WLgZ1iogVwDNVTnkP8K0ouQ+YKeng3ZNdtkTEUxHxUPnr5yn90R0y5jS3Z43KbfRCOewoP8auGHF71khSJ/DHwPUTnJLptnQxSN4hwIZRcR+vfYOzMSTNAY4H7h9zyO05CeVhjdXAFqA3Ityeu+4rwN8AQxMcz3RbuhgkT+N8z+t5q5A0A/gB8MmIeG7s4XF+xO05gYjYERHHAZ3AiZLeMuYUt2cNJJ0LbImIB6udNs73MtOWLgbJ6wMOHRV3AptSyqXpSeqgVAi+ExE/HOcUt+cuiIhtwJ28dn7L7VmbtwPvlrQe+C5whqRvjzkn023pYpC8ZcBHyysNTgZ+HxFPpZ1UM5Ik4JvA4xHxTxOc5vaskaRZkmaWv94LOBN4Ysxpbs8aRMTfRURnRMwBPgT8e0R8eMxpmW5L3wO5TpJuAU4HcpL6gCspTdQREdcCtwHnAOuA7cDH08k0E94OfAT4TXmcG+DTwGHg9twFBwMFSe2UPvh9LyJ+JmkRuD0boZXa0ttRmJmZh4nMzMzFwMzMcDEwMzNcDMzMDBcDMzPDxcDMzHAxMDMzXAzMGkLS50bff0HS5yVdmmZOZpPhi87MGqC8y+oPI+IESW3AWuDEiNiabmZmtfF2FGYNEBHrJW2VdDxwILDKhcCyxMXArHGuBz4GHATckG4qZpPjYSKzBpG0B/AbShsVHhkRO1JOyaxm7hmYNUhEvCrpDmCbC4FljYuBWYOUJ45PBt6fdi5mk+WlpWYNIOkYSvvY/zIi1qadj9lkec7AzMzcMzAzMxcDMzPDxcDMzHAxMDMzXAzMzAz4/yrglLDjDOuuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # visualize results\n",
    "results = pd.DataFrame(data=np.hstack((y_scale, y_scale_pred)),\n",
    "             columns=[\"y\", \"y_pred\"])\n",
    "\n",
    "sns.boxplot(data=results, x='y', y='y_pred').set(title='Prediction by Label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
