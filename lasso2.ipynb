{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f19a432",
   "metadata": {},
   "source": [
    "## CMSC 35300 Final Project: Lasso Models\n",
    "Shweta Kamath <br>\n",
    "Nivedita Vatsa <br>\n",
    "Carolyn Vilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44aa39",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152ce23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecafe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(\"data/all_data_standardized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035b2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out Xs\n",
    "X = df.loc[:, ~df.columns.isin([\"child_id\", \"mother_id\", \"treat_alike_scale\", \"treat_alike_binary\"])]\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Separate out two prospective ys\n",
    "y_scale = df.loc[:, df.columns == \"treat_alike_scale\"]\n",
    "y_scale = y_scale.to_numpy()\n",
    "\n",
    "y_binary = df.loc[:, df.columns == \"treat_alike_binary\"]\n",
    "y_binary = y_binary.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d07bb",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Predict repeatedly using cross validation; plot test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fc864",
   "metadata": {},
   "source": [
    "Source (blog + GitHub):\n",
    "* https://towardsdatascience.com/regularized-linear-regression-models-dcf5aa662ab9\n",
    "* https://gist.github.com/wyattowalsh/6a95b1c9ad6118b196336cffd5de4f72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e12320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(X, y, l1, tol=1e-6, path_length=100, return_path=False):\n",
    "    \"\"\"The Lasso Regression model with intercept term.\n",
    "    Intercept term included via design matrix augmentation.\n",
    "    Pathwise coordinate descent with co-variance updates is applied.\n",
    "    Path from max value of the L1 tuning parameter to input tuning parameter value.\n",
    "    Features must be standardized (centered and scaled to unit variance)\n",
    "    Params:\n",
    "        X - NumPy matrix, size (N, p), of standardized numerical predictors\n",
    "        y - NumPy array, length N, of numerical response\n",
    "        l1 - L1 penalty tuning parameter (positive scalar)\n",
    "        tol - Coordinate Descent convergence tolerance (exited if change < tol)\n",
    "        path_length - Number of tuning parameter values to include in path (positive integer)\n",
    "        return_path - Boolean indicating whether model coefficients along path should be returned\n",
    "    Returns:\n",
    "        if return_path == False:\n",
    "            NumPy array, length p + 1, of fitted model coefficients\n",
    "        if return_path == True:\n",
    "            List, length 3, of last fitted model coefficients, tuning parameter path and coefficient values\n",
    "    \"\"\"\n",
    "    X = np.hstack((np.ones((len(X), 1)), X))\n",
    "    m, n = np.shape(X)\n",
    "    B_star = np.zeros((n))\n",
    "    l_max = max(list(abs(np.dot(np.transpose(X[:, 1:]), y)))) / m\n",
    "    # At or above l_max, all coefficients (except intercept) will be brought to 0\n",
    "    if l1 >= l_max:\n",
    "        return np.append(np.mean(y), np.zeros((n - 1)))\n",
    "    l_path = np.geomspace(l_max, l1, path_length)\n",
    "    coeffiecients = np.zeros((len(l_path), n))\n",
    "    for i in range(len(l_path)):\n",
    "        while True:\n",
    "            B_s = B_star\n",
    "            for j in range(n):\n",
    "                k = np.where(B_s != 0)[0]\n",
    "                update = (1/m)*((np.dot(X[:,j], y)- \\\n",
    "                                np.dot(np.dot(X[:,j], X[:,k]), B_s[k]))) + \\\n",
    "                                B_s[j]\n",
    "                B_star[j] = (np.sign(update) * max(abs(update) - l_path[i], 0))\n",
    "            if np.all(abs(B_s - B_star) < tol):\n",
    "                coeffiecients[i, :] = B_star\n",
    "                break\n",
    "    if return_path:\n",
    "        return [B_star, l_path, coeffiecients]\n",
    "    else:\n",
    "        return B_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = lasso(X, y_scale, l1=0.1, tol=1e-6, path_length=10, return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60f8bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.hstack((np.ones((len(X), 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a45f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vec = rv[0].reshape(len(rv[0]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b58058bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.75332096e+149],\n",
       "       [-1.89954341e+151],\n",
       "       [ 5.11726785e+155],\n",
       "       [-1.42027326e+159],\n",
       "       [ 5.85572677e+164],\n",
       "       [-7.53499916e+167],\n",
       "       [-2.31662432e+168],\n",
       "       [ 1.01670780e+170],\n",
       "       [-2.57430171e+171],\n",
       "       [ 9.94662103e+171],\n",
       "       [-4.36748411e+172],\n",
       "       [ 1.81239033e+173],\n",
       "       [-9.45369670e+173],\n",
       "       [ 5.79481032e+174],\n",
       "       [-2.81822500e+175],\n",
       "       [ 1.23979097e+176],\n",
       "       [-7.40648440e+176],\n",
       "       [ 5.57228707e+177],\n",
       "       [-2.99609595e+178],\n",
       "       [ 1.49837860e+179],\n",
       "       [-9.78917627e+179],\n",
       "       [ 7.68455017e+180],\n",
       "       [-3.98837248e+181],\n",
       "       [ 2.04284559e+182],\n",
       "       [-6.09579665e+185],\n",
       "       [ 8.49306795e+189],\n",
       "       [-1.75883734e+190],\n",
       "       [-3.99865247e+195],\n",
       "       [ 3.32254675e+199],\n",
       "       [ 1.23859887e+200],\n",
       "       [ 5.97836911e+199],\n",
       "       [ 4.12248718e+198],\n",
       "       [ 3.50576406e+198],\n",
       "       [ 3.76690870e+199],\n",
       "       [ 4.18306818e+199],\n",
       "       [ 1.39670232e+198],\n",
       "       [ 5.96973049e+198],\n",
       "       [ 3.24818537e+199],\n",
       "       [ 1.12915878e+199],\n",
       "       [ 2.67866328e+199],\n",
       "       [ 2.83304569e+198],\n",
       "       [ 7.52482974e+198],\n",
       "       [ 1.06525419e+199],\n",
       "       [ 6.01914230e+198]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f001f43d",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480c0a0",
   "metadata": {},
   "source": [
    "Source: https://xavierbourretsicotte.github.io/lasso_implementation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b8b7754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== lambda: 0 ==========\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (5161,1) and (44,1) not aligned: 1 (dim 1) != 44 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10788/1945259715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mX_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mrho\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_scale\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_scale_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m#rho = X_j.T @ (y_scale - y_scale_pred + w[j]*X_j)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# print(rho)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5161,1) and (44,1) not aligned: 1 (dim 1) != 44 (dim 0)"
     ]
    }
   ],
   "source": [
    "m, n = X_.shape\n",
    "num_iters = 2\n",
    "w_list = []\n",
    "\n",
    "w = np.ones((n,1))\n",
    "lamda_list = [0]\n",
    "#lamda_list = np.logspace(0,4,300)/10 #Range of lambda values\n",
    "\n",
    "for lamda in lamda_list:\n",
    "    print(\"=\"*10,\"lambda:\",lamda,\"=\"*10)\n",
    "    for i in range(num_iters): # iteration\n",
    "        for j in range(n): # column\n",
    "            y_scale_pred = X_ @ w\n",
    "\n",
    "            X_j = X_[:,j].reshape(-1,1)\n",
    "            \n",
    "            rho = X_j.T @ (y_scale - y_scale_pred + w[j]*X_j)\n",
    "            # print(rho)\n",
    "            \n",
    "            # update weight vector\n",
    "            if j == 0: \n",
    "                w[j] = rho # intercept term\n",
    "            else:\n",
    "#                 if rho < (-lamda/2):\n",
    "#                     w[j] = rho + (lamda/2)\n",
    "#                 elif rho > (lamda/2):\n",
    "#                     w[j] = rho - (lamda/2)\n",
    "#                 else:\n",
    "#                     w[j] = 0\n",
    "                if rho < (-lamda):\n",
    "                    w[j] = rho + lamda\n",
    "                elif rho >  lamda:\n",
    "                    w[j] = rho - lamda\n",
    "                else:\n",
    "                    w[j] = 0\n",
    "        print(\"j:\", j)\n",
    "        print(\"w:\", w)\n",
    "        w_list.append(w)\n",
    "\n",
    "    \n",
    "#print(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lasso = np.stack(w_list).T\n",
    "X_names = df.columns[(~df.columns.isin([\"child_id\", \"mother_id\", \"treat_alike_scale\", \"treat_alike_binary\"]))]\n",
    "\n",
    "\n",
    "#Plot results\n",
    "n,_ = w_lasso.shape\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "for i in range(n):\n",
    "    plt.plot(lamda, theta_lasso[i], label = X_names[i])\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log($\\\\lambda$)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Paths - Numpy implementation')\n",
    "plt.legend()\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aab5231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(self, learning_rate, iteration, regularization):\n",
    "        \"\"\"\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        \"\"\"\n",
    "        self.m = None\n",
    "        self.n = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.regularization = regularization # will be the l1/l2 regularization class according to the regression model.\n",
    "        self.lr = learning_rate\n",
    "        self.it = iteration\n",
    "\n",
    "    def cost_function(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        :param y: Original target value.\n",
    "        :param y_pred: predicted target value.\n",
    "        \"\"\"\n",
    "        # return np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
    "        return (1 / (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
    "    \n",
    "    def hypothesis(self, weights, bias, X):\n",
    "        \"\"\"\n",
    "        :param weights: parameter value weight.\n",
    "        :param X: Training samples.\n",
    "        \"\"\"\n",
    "        return np.dot(X, weights) #+ bias\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Target value should be in the shape of (n, 1) not (n, ).\n",
    "        # So, this will check that and change the shape to (n, 1), if not.\n",
    "        try:\n",
    "            y.shape[1]\n",
    "        except IndexError as e:\n",
    "            # we need to change it to the 1 D array, not a list.\n",
    "            print(\"ERROR: Target array should be a one dimentional array not a list\"\n",
    "                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n",
    "                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n",
    "            return \n",
    "        \n",
    "        # m is the number of training samples.\n",
    "        self.m = X.shape[0]\n",
    "        # n is the number of features.\n",
    "        self.n = X.shape[1]\n",
    "\n",
    "        # Set the initial weight.\n",
    "        self.w = np.zeros((self.n , 1)) + 1\n",
    "\n",
    "        # bias.\n",
    "        self.b = 0\n",
    "\n",
    "        for it in range(1, self.it+1):\n",
    "            # 1. Find the predicted value through the hypothesis.\n",
    "            # 2. Find the Cost function value.\n",
    "            # 3. Find the derivation of weights.\n",
    "            # 4. Apply Gradient Decent.\n",
    "            y_pred = self.hypothesis(self.w, self.b, X)\n",
    "            #print(\"iteration\",it)\n",
    "            #print(\"y predict value\",y_pred)\n",
    "            cost = self.cost_function(y, y_pred)\n",
    "            #print(\"Cost function\",cost)\n",
    "            # fin the derivative.\n",
    "            dw = np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
    "            # dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
    "            #print(\"weights derivation\",dw)\n",
    "            #db = -(2 / self.m) * np.sum((y_pred - y))\n",
    "\n",
    "            # change the weight parameter.\n",
    "            self.w = self.w - self.lr * dw\n",
    "            #print(\"updated weights\",self.w)\n",
    "            #self.b = self.b - self.lr * db\n",
    "\n",
    "            if it % 10000 == 0:\n",
    "                print(\"The Cost function for the iteration {}----->{}\".format(it, cost))\n",
    "                \n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        :param test_X: feature values to predict.\n",
    "        \"\"\"\n",
    "        # Insert constant ones for bias weights.\n",
    "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
    "\n",
    "        y_pred = self.hypothesis(self.w, self.b, test_X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cec15dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class l1_regularization:\n",
    "    '''Regularization used for Lasso Regression'''\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        '''This will be retuned when we call this class.'''\n",
    "        return self.lamda * np.sum(np.abs(weights))\n",
    "    \n",
    "    def derivation(self, weights):\n",
    "        \"Derivation of the regulariozation function.\"\n",
    "        return self.lamda * np.sign(weights)\n",
    "    \n",
    "    \n",
    "class l2_regularization:\n",
    "    '''Regularization used for Ridge Regression'''\n",
    "    def __init__(self, lamda):\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        \"This will be retuned when we call this class.\"\n",
    "        return self.lamda * np.sum(np.square(weights))\n",
    "    \n",
    "    def derivation(self, weights):\n",
    "        \"Derivation of the regulariozation function.\"\n",
    "        return self.lamda * 2 * (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d1e58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(Regression):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, lamda, learning_rate, iteration):\n",
    "        '''\n",
    "        Define the hyperparameters we are going to use in this model.\n",
    "        :param lamda: Regularization factor.\n",
    "        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "        :param iteration: Number of training iteration, default value is 10,000.\n",
    "        '''\n",
    "        self.regularization = l1_regularization(lamda)\n",
    "        super(LassoRegression, self).__init__(learning_rate, iteration, self.regularization)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''\n",
    "        :param X: training data feature values ---> N Dimentional vector.\n",
    "        :param y: training data target value -----> 1 Dimentional array.\n",
    "        '''\n",
    "        return super(LassoRegression, self).train(X, y)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        '''\n",
    "        parma test_X: Value need to be predicted.\n",
    "        '''\n",
    "        return super(LassoRegression, self).predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24137c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RidgeRegression(Regression):\n",
    "#     '''\n",
    "#     Ridge Regression is one of the variance of the Linear Regression. This model doing the parameter learning \n",
    "#     and regularization at the same time. This model uses the l2-regularization. \n",
    "#     This is very similar to the Lasso regression.\n",
    "#     * Regularization will be one of the soluions to the Overfitting.\n",
    "#     * Overfitting happens when the model has \"High Variance and low bias\". So, regularization adds a little bias to the model.\n",
    "#     * This model will try to keep the balance between learning the parameters and the complexity of the model( tries to keep the parameter having small value and small degree of palinamial).\n",
    "#     * The Regularization parameter(lamda) controls how severe  the regularization is. \n",
    "#     * large lamda adds more bias , hence the Variance will go very small --> this may cause underfitting(Low bias and High Varinace).\n",
    "#     * Lamda can be found by tial and error methos. \n",
    "#     '''\n",
    "#     def __init__(self, lamda, learning_rate, iteration):\n",
    "#         \"\"\"\n",
    "#         Define the hyperparameters we are going to use in this model.\n",
    "#         :param lamda: Regularization factor.\n",
    "#         :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n",
    "#         :param iteration: Number of training iteration, default value is 10,000.\n",
    "#         \"\"\"\n",
    "#         self.regularization = l2_regularization(lamda)\n",
    "#         super(RidgeRegression, self).__init__(learning_rate, iteration, self.regularization)\n",
    "\n",
    "#     def train(self, X, y):\n",
    "#         \"\"\"\n",
    "#         :param X: training data feature values ---> N Dimentional vector.\n",
    "#         :param y: training data target value -----> 1 Dimentional array.\n",
    "#         \"\"\"\n",
    "#         return super(RidgeRegression, self).train(X, y)\n",
    "#     def predict(self, test_X):\n",
    "#         \"\"\"\n",
    "#         parma test_X: Value need to be predicted.\n",
    "#         \"\"\"\n",
    "#         return super(RidgeRegression, self).predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12ed6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_scale_labels(y_scale_pred):\n",
    "    '''\n",
    "    Assign label 1 through 4\n",
    "    \n",
    "    arg:\n",
    "    - y_scale_pred: vector of predicted y values\n",
    "    \n",
    "    Returns: vector of predicted y labels (1 through 4)\n",
    "    '''\n",
    "    # generate nx4 matrix where each column is 1,2,3,4\n",
    "    m, _ = y_scale_pred.shape\n",
    "    mat = np.tile(np.arange(4)+1, (m,1))\n",
    "    \n",
    "    # find lowest absolute distance\n",
    "    return np.argmin(abs(mat - y_scale_pred), axis=1).reshape(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1be30",
   "metadata": {},
   "source": [
    "### Estimate Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c6ca888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cost function for the iteration 10----->2959462394.2005224\n",
      "The Cost function for the iteration 20----->2959462231.4092817\n",
      "The Cost function for the iteration 30----->2959462068.6180515\n",
      "The Cost function for the iteration 40----->2959461905.826829\n",
      "The Cost function for the iteration 50----->2959461743.0356145\n",
      "The Cost function for the iteration 60----->2959461580.24441\n",
      "The Cost function for the iteration 70----->2959461417.4532146\n",
      "The Cost function for the iteration 80----->2959461254.662028\n",
      "The Cost function for the iteration 90----->2959461091.8708496\n",
      "The Cost function for the iteration 100----->2959460929.079681\n",
      "The Cost function for the iteration 110----->2959460766.288522\n",
      "The Cost function for the iteration 120----->2959460603.4973702\n",
      "The Cost function for the iteration 130----->2959460440.7062297\n",
      "The Cost function for the iteration 140----->2959460277.9150968\n",
      "The Cost function for the iteration 150----->2959460115.123973\n",
      "The Cost function for the iteration 160----->2959459952.332858\n",
      "The Cost function for the iteration 170----->2959459789.541753\n",
      "The Cost function for the iteration 180----->2959459626.7506566\n",
      "The Cost function for the iteration 190----->2959459463.959567\n",
      "The Cost function for the iteration 200----->2959459301.168489\n",
      "The Cost function for the iteration 210----->2959459138.377419\n",
      "The Cost function for the iteration 220----->2959458975.586357\n",
      "The Cost function for the iteration 230----->2959458812.7953053\n",
      "The Cost function for the iteration 240----->2959458650.0042634\n",
      "The Cost function for the iteration 250----->2959458487.213229\n",
      "The Cost function for the iteration 260----->2959458324.4222035\n",
      "The Cost function for the iteration 270----->2959458161.6311874\n",
      "The Cost function for the iteration 280----->2959457998.840182\n",
      "The Cost function for the iteration 290----->2959457836.0491834\n",
      "The Cost function for the iteration 300----->2959457673.258193\n",
      "The Cost function for the iteration 310----->2959457510.4672146\n",
      "The Cost function for the iteration 320----->2959457347.6762433\n",
      "The Cost function for the iteration 330----->2959457184.8852806\n",
      "The Cost function for the iteration 340----->2959457022.0943274\n",
      "The Cost function for the iteration 350----->2959456859.303384\n",
      "The Cost function for the iteration 360----->2959456696.512449\n",
      "The Cost function for the iteration 370----->2959456533.721522\n",
      "The Cost function for the iteration 380----->2959456370.9306054\n",
      "The Cost function for the iteration 390----->2959456208.1396976\n",
      "The Cost function for the iteration 400----->2959456045.3487983\n",
      "The Cost function for the iteration 410----->2959455882.557907\n",
      "The Cost function for the iteration 420----->2959455719.767027\n",
      "The Cost function for the iteration 430----->2959455556.9761543\n",
      "The Cost function for the iteration 440----->2959455394.18529\n",
      "The Cost function for the iteration 450----->2959455231.394437\n",
      "The Cost function for the iteration 460----->2959455068.603591\n",
      "The Cost function for the iteration 470----->2959454905.812754\n",
      "The Cost function for the iteration 480----->2959454743.0219264\n",
      "The Cost function for the iteration 490----->2959454580.2311087\n",
      "The Cost function for the iteration 500----->2959454417.4403\n"
     ]
    }
   ],
   "source": [
    "param = {\"lamda\" : 0,\n",
    "         \"learning_rate\" : 0.0000000000000000000001,\n",
    "         \"iteration\" : 500}\n",
    "\n",
    "lasso = LassoRegression(**param)\n",
    "lasso.train(X, y_scale)\n",
    "\n",
    "# predict\n",
    "y_scale_pred = lasso.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67163e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = df.columns[(~df.columns.isin([\"child_id\", \"mother_id\", \"treat_alike_scale\", \"treat_alike_binary\"]))]\n",
    "\n",
    "filt = (np.round(lasso.w[1:], 4)>0.0).flatten()\n",
    "b_names = X_names[filt]\n",
    "b_vals  = lasso.w[1:][filt]\n",
    "\n",
    "print(\"---most important features---\")\n",
    "for n, v in zip(b_names, b_vals):\n",
    "    print(n, \":\", '%.08f' % v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---error rate---\")\n",
    "y_scale_pred_label = predict_scale_labels(y_scale_pred)\n",
    "print(100*np.sum(y_scale != y_scale_pred_label)/len(y_scale))\n",
    "\n",
    "print(\"---mean square error ---\")\n",
    "mse = (np.square(y_scale - y_scale_pred)).mean(axis=None)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize results\n",
    "results = pd.DataFrame(data=np.hstack((y_scale, y_scale_pred)),\n",
    "             columns=[\"y\", \"y_pred\"])\n",
    "\n",
    "sns.boxplot(data=results, x='y', y='y_pred').set(title='Prediction by Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd927c2",
   "metadata": {},
   "source": [
    "# Lasso Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_sklearn = Lasso(alpha=0.0)\n",
    "lasso_sklearn.fit(X, y_scale)\n",
    "\n",
    "# predict the value\n",
    "y_pred_sklearn = lasso_sklearn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_sklearn.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d93378",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(y_pred_sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
